{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "improve attempt",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1hYed6uFddQm-ufCdZibr2kyhczFk7NEh",
      "authorship_tag": "ABX9TyNArVGhluBHKDhPKt413tyt"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9StSylLnR4",
        "colab_type": "text"
      },
      "source": [
        "# Inital imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cilpDcRnDZJp",
        "colab_type": "code",
        "outputId": "30a19ac0-f3a4-45c7-b136-6023cc963464",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "pip install imageai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting imageai\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/99/4023e191a343fb23f01ae02ac57a5ca58037c310e8d8c62f87638a3bafc7/imageai-2.1.5-py3-none-any.whl (180kB)\n",
            "\r\u001b[K     |█▉                              | 10kB 28.2MB/s eta 0:00:01\r\u001b[K     |███▋                            | 20kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 30kB 6.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 40kB 5.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 61kB 6.3MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 71kB 6.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 81kB 7.2MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 92kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 102kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 112kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 122kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 133kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 143kB 7.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 153kB 7.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 163kB 7.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 174kB 7.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 184kB 7.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageai) (7.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from imageai) (3.2.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageai) (1.18.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from imageai) (2.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->imageai) (2.4.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->imageai) (1.12.0)\n",
            "Installing collected packages: imageai\n",
            "Successfully installed imageai-2.1.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnZ6vRR2C92A",
        "colab_type": "code",
        "outputId": "58cf4f02-24b1-490f-860b-8089d43c4bae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "!pip3 install tensorflow-gpu==1.13.1"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 24kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.27.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.9.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.10.0)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /simple/tensorboard/\u001b[0m\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 37.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.18.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 54.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.34.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.3.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (46.1.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.2.1)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/74/d72daf8dff5b6566db857cfd088907bb0355f5dd2914c4b3ef065c790735/mock-4.0.2-py3-none-any.whl\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorboard<2.3.0,>=2.2.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.2.0rc2 has requirement tensorflow-estimator<2.3.0,>=2.2.0rc0, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, mock, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "Successfully installed mock-4.0.2 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DodenSlgNTbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import imageai"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBZ3O-U5DBfX",
        "colab_type": "code",
        "outputId": "841137bf-88a6-4ea8-eb8d-0e6d51f09106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "from imageai.Detection.Custom import DetectionModelTrainer # will tell you to upgrade to tf2.x, ignore as this won't work in 2.x"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktvWmZ3qEGIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzTFi0SxFSEo",
        "colab_type": "code",
        "outputId": "fe005cac-d562-448c-dfc4-87fe7ade8753",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.path.abspath(\"drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\") # grab absolute paths just in case can't find by relative path (dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5l1mi0DFR9m",
        "colab_type": "code",
        "outputId": "f77cd362-1ff3-4a2e-c666-12aa9ba2ed84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "os.path.abspath(\"drive/My Drive/Object Detection Final Project/obj detect 2/pretrained-yolov3.h5\") # grab absolute paths just in case can't find by relative path (model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/My Drive/Object Detection Final Project/obj detect 2/pretrained-yolov3.h5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEH_D-wmFR1L",
        "colab_type": "text"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS_XGqlQDXl7",
        "colab_type": "code",
        "outputId": "a331c602-f3c0-4d56-b75f-27876696236a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "trainer.setTrainConfig(object_names_array=[\"toiletpaper\"], # array of classes\n",
        "                      #  batch_size=4, \n",
        "                       num_experiments=200, # epochs\n",
        "                       train_from_pretrained_model=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/pretrained-yolov3.h5\")\n",
        "trainer.trainModel()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generating anchor boxes for training images and annotation...\n",
            "Average IOU for 9 anchors: 0.75\n",
            "Anchor Boxes generated.\n",
            "Detection configuration saved in  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\n",
            "Training on: \t['toiletpaper']\n",
            "Training with Batch Size:  4\n",
            "Number of Experiments:  200\n",
            "Training with transfer learning from pretrained Model\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "  warnings.warn('`epsilon` argument is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "168/168 [==============================] - 253s 2s/step - loss: 65.0175 - yolo_layer_7_loss: 9.9568 - yolo_layer_8_loss: 15.1132 - yolo_layer_9_loss: 39.9474 - val_loss: 33.0861 - val_yolo_layer_7_loss: 6.4393 - val_yolo_layer_8_loss: 8.5934 - val_yolo_layer_9_loss: 18.0535\n",
            "Epoch 2/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 28.1874 - yolo_layer_7_loss: 5.7133 - yolo_layer_8_loss: 8.1586 - yolo_layer_9_loss: 14.3155 - val_loss: 22.9075 - val_yolo_layer_7_loss: 4.7112 - val_yolo_layer_8_loss: 5.8191 - val_yolo_layer_9_loss: 12.3773\n",
            "Epoch 3/200\n",
            "168/168 [==============================] - 170s 1s/step - loss: 22.2553 - yolo_layer_7_loss: 4.4536 - yolo_layer_8_loss: 6.7823 - yolo_layer_9_loss: 11.0194 - val_loss: 19.9833 - val_yolo_layer_7_loss: 4.4528 - val_yolo_layer_8_loss: 5.3593 - val_yolo_layer_9_loss: 10.1712\n",
            "Epoch 4/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 20.4834 - yolo_layer_7_loss: 4.4410 - yolo_layer_8_loss: 6.8760 - yolo_layer_9_loss: 9.1663 - val_loss: 20.0135 - val_yolo_layer_7_loss: 4.4128 - val_yolo_layer_8_loss: 5.4199 - val_yolo_layer_9_loss: 10.1808\n",
            "Epoch 5/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 18.4360 - yolo_layer_7_loss: 3.9325 - yolo_layer_8_loss: 5.8140 - yolo_layer_9_loss: 8.6895 - val_loss: 21.9148 - val_yolo_layer_7_loss: 4.7569 - val_yolo_layer_8_loss: 5.9487 - val_yolo_layer_9_loss: 11.2092\n",
            "Epoch 6/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 17.1484 - yolo_layer_7_loss: 2.9525 - yolo_layer_8_loss: 5.8219 - yolo_layer_9_loss: 8.3739 - val_loss: 17.4074 - val_yolo_layer_7_loss: 3.3630 - val_yolo_layer_8_loss: 4.4385 - val_yolo_layer_9_loss: 9.6060\n",
            "Epoch 7/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 15.0788 - yolo_layer_7_loss: 2.0160 - yolo_layer_8_loss: 5.2059 - yolo_layer_9_loss: 7.8570 - val_loss: 18.0060 - val_yolo_layer_7_loss: 3.4402 - val_yolo_layer_8_loss: 4.3594 - val_yolo_layer_9_loss: 10.2064\n",
            "Epoch 8/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 12.1672 - yolo_layer_7_loss: 1.8353 - yolo_layer_8_loss: 3.6685 - yolo_layer_9_loss: 6.6634 - val_loss: 18.0735 - val_yolo_layer_7_loss: 3.9634 - val_yolo_layer_8_loss: 4.3901 - val_yolo_layer_9_loss: 9.7200\n",
            "Epoch 9/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 11.1857 - yolo_layer_7_loss: 1.6599 - yolo_layer_8_loss: 3.0474 - yolo_layer_9_loss: 6.4784 - val_loss: 18.8614 - val_yolo_layer_7_loss: 3.8005 - val_yolo_layer_8_loss: 4.7924 - val_yolo_layer_9_loss: 10.2685\n",
            "Epoch 10/200\n",
            "168/168 [==============================] - 174s 1s/step - loss: 11.6855 - yolo_layer_7_loss: 1.6986 - yolo_layer_8_loss: 3.5925 - yolo_layer_9_loss: 6.3944 - val_loss: 17.9821 - val_yolo_layer_7_loss: 3.0860 - val_yolo_layer_8_loss: 3.8564 - val_yolo_layer_9_loss: 11.0397\n",
            "Epoch 11/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 10.7888 - yolo_layer_7_loss: 1.7759 - yolo_layer_8_loss: 3.2205 - yolo_layer_9_loss: 5.7925 - val_loss: 16.1436 - val_yolo_layer_7_loss: 2.6500 - val_yolo_layer_8_loss: 4.4296 - val_yolo_layer_9_loss: 9.0640\n",
            "Epoch 12/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 10.9733 - yolo_layer_7_loss: 1.7909 - yolo_layer_8_loss: 2.9091 - yolo_layer_9_loss: 6.2733 - val_loss: 16.4728 - val_yolo_layer_7_loss: 3.6260 - val_yolo_layer_8_loss: 3.7760 - val_yolo_layer_9_loss: 9.0708\n",
            "Epoch 13/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 9.5289 - yolo_layer_7_loss: 1.3716 - yolo_layer_8_loss: 2.8367 - yolo_layer_9_loss: 5.3206 - val_loss: 16.7739 - val_yolo_layer_7_loss: 2.9912 - val_yolo_layer_8_loss: 3.8885 - val_yolo_layer_9_loss: 9.8941\n",
            "Epoch 14/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 9.2694 - yolo_layer_7_loss: 1.4691 - yolo_layer_8_loss: 2.8704 - yolo_layer_9_loss: 4.9298 - val_loss: 17.1842 - val_yolo_layer_7_loss: 4.5225 - val_yolo_layer_8_loss: 2.9656 - val_yolo_layer_9_loss: 9.6961\n",
            "Epoch 15/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 9.2251 - yolo_layer_7_loss: 1.4822 - yolo_layer_8_loss: 3.1079 - yolo_layer_9_loss: 4.6349 - val_loss: 16.0233 - val_yolo_layer_7_loss: 3.1294 - val_yolo_layer_8_loss: 3.4502 - val_yolo_layer_9_loss: 9.4436\n",
            "Epoch 16/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 8.6953 - yolo_layer_7_loss: 1.1833 - yolo_layer_8_loss: 2.4557 - yolo_layer_9_loss: 5.0563 - val_loss: 16.7441 - val_yolo_layer_7_loss: 3.4194 - val_yolo_layer_8_loss: 4.4680 - val_yolo_layer_9_loss: 8.8567\n",
            "Epoch 17/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 9.0291 - yolo_layer_7_loss: 1.2916 - yolo_layer_8_loss: 2.4695 - yolo_layer_9_loss: 5.2680 - val_loss: 15.5483 - val_yolo_layer_7_loss: 2.8241 - val_yolo_layer_8_loss: 3.7157 - val_yolo_layer_9_loss: 9.0085\n",
            "Epoch 18/200\n",
            "168/168 [==============================] - 170s 1s/step - loss: 8.1174 - yolo_layer_7_loss: 0.9441 - yolo_layer_8_loss: 2.4012 - yolo_layer_9_loss: 4.7721 - val_loss: 15.7008 - val_yolo_layer_7_loss: 2.9946 - val_yolo_layer_8_loss: 4.3572 - val_yolo_layer_9_loss: 8.3490\n",
            "Epoch 19/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 8.0604 - yolo_layer_7_loss: 1.2380 - yolo_layer_8_loss: 2.5319 - yolo_layer_9_loss: 4.2904 - val_loss: 15.3964 - val_yolo_layer_7_loss: 1.9394 - val_yolo_layer_8_loss: 4.6406 - val_yolo_layer_9_loss: 8.8163\n",
            "Epoch 20/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 8.2048 - yolo_layer_7_loss: 1.0893 - yolo_layer_8_loss: 2.5562 - yolo_layer_9_loss: 4.5593 - val_loss: 16.7984 - val_yolo_layer_7_loss: 3.4596 - val_yolo_layer_8_loss: 3.7879 - val_yolo_layer_9_loss: 9.5509\n",
            "Epoch 21/200\n",
            "168/168 [==============================] - 175s 1s/step - loss: 7.6478 - yolo_layer_7_loss: 1.1520 - yolo_layer_8_loss: 2.3087 - yolo_layer_9_loss: 4.1871 - val_loss: 15.8373 - val_yolo_layer_7_loss: 2.9547 - val_yolo_layer_8_loss: 4.6742 - val_yolo_layer_9_loss: 8.2084\n",
            "Epoch 22/200\n",
            "168/168 [==============================] - 174s 1s/step - loss: 7.7709 - yolo_layer_7_loss: 1.0923 - yolo_layer_8_loss: 2.3815 - yolo_layer_9_loss: 4.2971 - val_loss: 16.0185 - val_yolo_layer_7_loss: 2.6848 - val_yolo_layer_8_loss: 4.1171 - val_yolo_layer_9_loss: 9.2166\n",
            "Epoch 23/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 7.3675 - yolo_layer_7_loss: 1.1044 - yolo_layer_8_loss: 2.4500 - yolo_layer_9_loss: 3.8131 - val_loss: 17.1302 - val_yolo_layer_7_loss: 3.7116 - val_yolo_layer_8_loss: 4.0825 - val_yolo_layer_9_loss: 9.3362\n",
            "Epoch 24/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 6.9447 - yolo_layer_7_loss: 0.9600 - yolo_layer_8_loss: 2.1147 - yolo_layer_9_loss: 3.8699 - val_loss: 17.1239 - val_yolo_layer_7_loss: 3.1753 - val_yolo_layer_8_loss: 4.2663 - val_yolo_layer_9_loss: 9.6823\n",
            "Epoch 25/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 7.2987 - yolo_layer_7_loss: 1.1332 - yolo_layer_8_loss: 2.2522 - yolo_layer_9_loss: 3.9133 - val_loss: 16.1016 - val_yolo_layer_7_loss: 3.7259 - val_yolo_layer_8_loss: 3.3697 - val_yolo_layer_9_loss: 9.0060\n",
            "Epoch 26/200\n",
            "168/168 [==============================] - 168s 999ms/step - loss: 7.0259 - yolo_layer_7_loss: 0.8573 - yolo_layer_8_loss: 2.1170 - yolo_layer_9_loss: 4.0516 - val_loss: 17.0629 - val_yolo_layer_7_loss: 3.9230 - val_yolo_layer_8_loss: 3.6881 - val_yolo_layer_9_loss: 9.4518\n",
            "Epoch 27/200\n",
            "168/168 [==============================] - 170s 1s/step - loss: 6.2634 - yolo_layer_7_loss: 0.8053 - yolo_layer_8_loss: 1.8914 - yolo_layer_9_loss: 3.5667 - val_loss: 16.3074 - val_yolo_layer_7_loss: 2.5328 - val_yolo_layer_8_loss: 3.7643 - val_yolo_layer_9_loss: 10.0103\n",
            "Epoch 28/200\n",
            "168/168 [==============================] - 175s 1s/step - loss: 6.1128 - yolo_layer_7_loss: 0.9406 - yolo_layer_8_loss: 1.9387 - yolo_layer_9_loss: 3.2334 - val_loss: 14.9346 - val_yolo_layer_7_loss: 2.3772 - val_yolo_layer_8_loss: 3.1303 - val_yolo_layer_9_loss: 9.4271\n",
            "Epoch 29/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 5.8065 - yolo_layer_7_loss: 0.6150 - yolo_layer_8_loss: 1.6892 - yolo_layer_9_loss: 3.5023 - val_loss: 16.7886 - val_yolo_layer_7_loss: 3.1834 - val_yolo_layer_8_loss: 3.9794 - val_yolo_layer_9_loss: 9.6258\n",
            "Epoch 30/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 6.1659 - yolo_layer_7_loss: 0.6995 - yolo_layer_8_loss: 2.0114 - yolo_layer_9_loss: 3.4550 - val_loss: 16.5325 - val_yolo_layer_7_loss: 2.6483 - val_yolo_layer_8_loss: 3.7464 - val_yolo_layer_9_loss: 10.1378\n",
            "Epoch 31/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 5.4835 - yolo_layer_7_loss: 0.7749 - yolo_layer_8_loss: 1.7078 - yolo_layer_9_loss: 3.0008 - val_loss: 16.7349 - val_yolo_layer_7_loss: 3.2676 - val_yolo_layer_8_loss: 3.5895 - val_yolo_layer_9_loss: 9.8778\n",
            "Epoch 32/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 5.3132 - yolo_layer_7_loss: 0.7853 - yolo_layer_8_loss: 1.5830 - yolo_layer_9_loss: 2.9449 - val_loss: 16.5658 - val_yolo_layer_7_loss: 3.0570 - val_yolo_layer_8_loss: 3.8464 - val_yolo_layer_9_loss: 9.6624\n",
            "Epoch 33/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 5.4498 - yolo_layer_7_loss: 0.6386 - yolo_layer_8_loss: 1.6093 - yolo_layer_9_loss: 3.2019 - val_loss: 15.7424 - val_yolo_layer_7_loss: 2.2640 - val_yolo_layer_8_loss: 3.3868 - val_yolo_layer_9_loss: 10.0916\n",
            "Epoch 34/200\n",
            "168/168 [==============================] - 170s 1s/step - loss: 5.5473 - yolo_layer_7_loss: 0.5980 - yolo_layer_8_loss: 1.6629 - yolo_layer_9_loss: 3.2864 - val_loss: 17.5566 - val_yolo_layer_7_loss: 3.1909 - val_yolo_layer_8_loss: 4.1232 - val_yolo_layer_9_loss: 10.2425\n",
            "Epoch 35/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 5.5607 - yolo_layer_7_loss: 0.6157 - yolo_layer_8_loss: 1.7416 - yolo_layer_9_loss: 3.2035 - val_loss: 16.9960 - val_yolo_layer_7_loss: 3.0847 - val_yolo_layer_8_loss: 3.9626 - val_yolo_layer_9_loss: 9.9487\n",
            "Epoch 36/200\n",
            "168/168 [==============================] - 169s 1s/step - loss: 5.1656 - yolo_layer_7_loss: 0.5652 - yolo_layer_8_loss: 1.2812 - yolo_layer_9_loss: 3.3193 - val_loss: 15.7682 - val_yolo_layer_7_loss: 2.4825 - val_yolo_layer_8_loss: 3.7253 - val_yolo_layer_9_loss: 9.5605\n",
            "Epoch 37/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 5.3128 - yolo_layer_7_loss: 0.5816 - yolo_layer_8_loss: 1.6509 - yolo_layer_9_loss: 3.0803 - val_loss: 15.3556 - val_yolo_layer_7_loss: 2.4143 - val_yolo_layer_8_loss: 3.2724 - val_yolo_layer_9_loss: 9.6689\n",
            "Epoch 38/200\n",
            "168/168 [==============================] - 168s 999ms/step - loss: 5.1453 - yolo_layer_7_loss: 0.4613 - yolo_layer_8_loss: 1.5089 - yolo_layer_9_loss: 3.1751 - val_loss: 16.1789 - val_yolo_layer_7_loss: 2.9114 - val_yolo_layer_8_loss: 3.7792 - val_yolo_layer_9_loss: 9.4883\n",
            "Epoch 39/200\n",
            "168/168 [==============================] - 170s 1s/step - loss: 5.6740 - yolo_layer_7_loss: 0.5919 - yolo_layer_8_loss: 1.3588 - yolo_layer_9_loss: 3.7233 - val_loss: 15.8782 - val_yolo_layer_7_loss: 2.5515 - val_yolo_layer_8_loss: 3.8208 - val_yolo_layer_9_loss: 9.5059\n",
            "Epoch 40/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 5.5499 - yolo_layer_7_loss: 0.5846 - yolo_layer_8_loss: 1.5463 - yolo_layer_9_loss: 3.4189 - val_loss: 16.0180 - val_yolo_layer_7_loss: 3.0354 - val_yolo_layer_8_loss: 4.0423 - val_yolo_layer_9_loss: 8.9402\n",
            "Epoch 41/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 6.0914 - yolo_layer_7_loss: 0.5821 - yolo_layer_8_loss: 2.0459 - yolo_layer_9_loss: 3.4634 - val_loss: 14.8757 - val_yolo_layer_7_loss: 2.4261 - val_yolo_layer_8_loss: 3.7577 - val_yolo_layer_9_loss: 8.6920\n",
            "Epoch 42/200\n",
            "168/168 [==============================] - 171s 1s/step - loss: 5.4105 - yolo_layer_7_loss: 0.5332 - yolo_layer_8_loss: 1.5902 - yolo_layer_9_loss: 3.2871 - val_loss: 16.7410 - val_yolo_layer_7_loss: 3.0649 - val_yolo_layer_8_loss: 4.3095 - val_yolo_layer_9_loss: 9.3666\n",
            "Epoch 43/200\n",
            "168/168 [==============================] - 173s 1s/step - loss: 5.3194 - yolo_layer_7_loss: 0.5685 - yolo_layer_8_loss: 1.6358 - yolo_layer_9_loss: 3.1152 - val_loss: 15.9787 - val_yolo_layer_7_loss: 2.9547 - val_yolo_layer_8_loss: 3.3396 - val_yolo_layer_9_loss: 9.6844\n",
            "Epoch 44/200\n",
            "168/168 [==============================] - 168s 1s/step - loss: 5.1299 - yolo_layer_7_loss: 0.5311 - yolo_layer_8_loss: 1.4578 - yolo_layer_9_loss: 3.1410 - val_loss: 17.1879 - val_yolo_layer_7_loss: 3.1171 - val_yolo_layer_8_loss: 4.6871 - val_yolo_layer_9_loss: 9.3837\n",
            "Epoch 45/200\n",
            "168/168 [==============================] - 174s 1s/step - loss: 5.5028 - yolo_layer_7_loss: 0.5649 - yolo_layer_8_loss: 1.5462 - yolo_layer_9_loss: 3.3918 - val_loss: 16.2922 - val_yolo_layer_7_loss: 2.3549 - val_yolo_layer_8_loss: 4.0470 - val_yolo_layer_9_loss: 9.8903\n",
            "Epoch 46/200\n",
            "168/168 [==============================] - 172s 1s/step - loss: 5.6936 - yolo_layer_7_loss: 0.5617 - yolo_layer_8_loss: 1.7358 - yolo_layer_9_loss: 3.3961 - val_loss: 17.9593 - val_yolo_layer_7_loss: 3.3358 - val_yolo_layer_8_loss: 4.1158 - val_yolo_layer_9_loss: 10.5077\n",
            "Epoch 47/200\n",
            "168/168 [==============================] - 178s 1s/step - loss: 5.4478 - yolo_layer_7_loss: 0.5360 - yolo_layer_8_loss: 1.6647 - yolo_layer_9_loss: 3.2470 - val_loss: 15.6320 - val_yolo_layer_7_loss: 2.5240 - val_yolo_layer_8_loss: 4.5910 - val_yolo_layer_9_loss: 8.5170\n",
            "Epoch 48/200\n",
            "168/168 [==============================] - 176s 1s/step - loss: 5.4205 - yolo_layer_7_loss: 0.6137 - yolo_layer_8_loss: 1.7714 - yolo_layer_9_loss: 3.0354 - val_loss: 16.5095 - val_yolo_layer_7_loss: 2.2938 - val_yolo_layer_8_loss: 4.2641 - val_yolo_layer_9_loss: 9.9516\n",
            "Epoch 49/200\n",
            "168/168 [==============================] - 177s 1s/step - loss: 5.5565 - yolo_layer_7_loss: 0.4716 - yolo_layer_8_loss: 1.5408 - yolo_layer_9_loss: 3.5441 - val_loss: 16.0562 - val_yolo_layer_7_loss: 2.5860 - val_yolo_layer_8_loss: 3.5159 - val_yolo_layer_9_loss: 9.9543\n",
            "Epoch 50/200\n",
            "168/168 [==============================] - 178s 1s/step - loss: 5.4604 - yolo_layer_7_loss: 0.6243 - yolo_layer_8_loss: 1.6164 - yolo_layer_9_loss: 3.2197 - val_loss: 15.2619 - val_yolo_layer_7_loss: 2.6405 - val_yolo_layer_8_loss: 4.3060 - val_yolo_layer_9_loss: 8.3154\n",
            "Epoch 51/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.6356 - yolo_layer_7_loss: 0.6555 - yolo_layer_8_loss: 1.7015 - yolo_layer_9_loss: 3.2786 - val_loss: 17.3940 - val_yolo_layer_7_loss: 3.4637 - val_yolo_layer_8_loss: 4.1872 - val_yolo_layer_9_loss: 9.7431\n",
            "Epoch 52/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.5298 - yolo_layer_7_loss: 0.5139 - yolo_layer_8_loss: 1.5832 - yolo_layer_9_loss: 3.4326 - val_loss: 16.2487 - val_yolo_layer_7_loss: 2.4596 - val_yolo_layer_8_loss: 4.2548 - val_yolo_layer_9_loss: 9.5343\n",
            "Epoch 53/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.4106 - yolo_layer_7_loss: 0.5617 - yolo_layer_8_loss: 1.6462 - yolo_layer_9_loss: 3.2027 - val_loss: 16.2783 - val_yolo_layer_7_loss: 2.6963 - val_yolo_layer_8_loss: 3.8294 - val_yolo_layer_9_loss: 9.7526\n",
            "Epoch 54/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 4.7114 - yolo_layer_7_loss: 0.4987 - yolo_layer_8_loss: 1.5093 - yolo_layer_9_loss: 2.7034 - val_loss: 15.9974 - val_yolo_layer_7_loss: 3.0696 - val_yolo_layer_8_loss: 3.4944 - val_yolo_layer_9_loss: 9.4335\n",
            "Epoch 55/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.7076 - yolo_layer_7_loss: 0.6550 - yolo_layer_8_loss: 1.8537 - yolo_layer_9_loss: 3.1990 - val_loss: 16.7107 - val_yolo_layer_7_loss: 2.8722 - val_yolo_layer_8_loss: 3.9125 - val_yolo_layer_9_loss: 9.9259\n",
            "Epoch 56/200\n",
            "168/168 [==============================] - 182s 1s/step - loss: 5.6371 - yolo_layer_7_loss: 0.6168 - yolo_layer_8_loss: 1.7617 - yolo_layer_9_loss: 3.2585 - val_loss: 16.4080 - val_yolo_layer_7_loss: 2.5277 - val_yolo_layer_8_loss: 4.0775 - val_yolo_layer_9_loss: 9.8028\n",
            "Epoch 57/200\n",
            "168/168 [==============================] - 182s 1s/step - loss: 5.6831 - yolo_layer_7_loss: 0.6276 - yolo_layer_8_loss: 1.8571 - yolo_layer_9_loss: 3.1983 - val_loss: 15.3612 - val_yolo_layer_7_loss: 2.6791 - val_yolo_layer_8_loss: 3.5017 - val_yolo_layer_9_loss: 9.1804\n",
            "Epoch 58/200\n",
            "168/168 [==============================] - 177s 1s/step - loss: 5.0211 - yolo_layer_7_loss: 0.5635 - yolo_layer_8_loss: 1.4180 - yolo_layer_9_loss: 3.0396 - val_loss: 17.8128 - val_yolo_layer_7_loss: 3.3643 - val_yolo_layer_8_loss: 3.6829 - val_yolo_layer_9_loss: 10.7656\n",
            "Epoch 59/200\n",
            "168/168 [==============================] - 181s 1s/step - loss: 5.3802 - yolo_layer_7_loss: 0.4606 - yolo_layer_8_loss: 1.5994 - yolo_layer_9_loss: 3.3203 - val_loss: 15.2903 - val_yolo_layer_7_loss: 2.7665 - val_yolo_layer_8_loss: 3.6083 - val_yolo_layer_9_loss: 8.9154\n",
            "Epoch 60/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.5341 - yolo_layer_7_loss: 0.6000 - yolo_layer_8_loss: 1.5862 - yolo_layer_9_loss: 3.3479 - val_loss: 14.8114 - val_yolo_layer_7_loss: 2.2987 - val_yolo_layer_8_loss: 3.1693 - val_yolo_layer_9_loss: 9.3435\n",
            "Epoch 61/200\n",
            "168/168 [==============================] - 182s 1s/step - loss: 5.5998 - yolo_layer_7_loss: 0.5521 - yolo_layer_8_loss: 1.7662 - yolo_layer_9_loss: 3.2815 - val_loss: 16.1250 - val_yolo_layer_7_loss: 2.1947 - val_yolo_layer_8_loss: 3.4202 - val_yolo_layer_9_loss: 10.5101\n",
            "Epoch 62/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.3713 - yolo_layer_7_loss: 0.4697 - yolo_layer_8_loss: 1.6716 - yolo_layer_9_loss: 3.2300 - val_loss: 16.8375 - val_yolo_layer_7_loss: 3.5127 - val_yolo_layer_8_loss: 4.0416 - val_yolo_layer_9_loss: 9.2832\n",
            "Epoch 63/200\n",
            "168/168 [==============================] - 181s 1s/step - loss: 5.2953 - yolo_layer_7_loss: 0.5824 - yolo_layer_8_loss: 1.5602 - yolo_layer_9_loss: 3.1527 - val_loss: 15.5503 - val_yolo_layer_7_loss: 2.6165 - val_yolo_layer_8_loss: 2.9662 - val_yolo_layer_9_loss: 9.9676\n",
            "Epoch 64/200\n",
            "168/168 [==============================] - 182s 1s/step - loss: 5.3750 - yolo_layer_7_loss: 0.6257 - yolo_layer_8_loss: 1.7248 - yolo_layer_9_loss: 3.0245 - val_loss: 15.8520 - val_yolo_layer_7_loss: 2.8156 - val_yolo_layer_8_loss: 3.7503 - val_yolo_layer_9_loss: 9.2862\n",
            "Epoch 65/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.1072 - yolo_layer_7_loss: 0.5771 - yolo_layer_8_loss: 1.4317 - yolo_layer_9_loss: 3.0984 - val_loss: 16.2631 - val_yolo_layer_7_loss: 2.9000 - val_yolo_layer_8_loss: 3.5257 - val_yolo_layer_9_loss: 9.8375\n",
            "Epoch 66/200\n",
            "168/168 [==============================] - 181s 1s/step - loss: 5.5406 - yolo_layer_7_loss: 0.6047 - yolo_layer_8_loss: 1.7889 - yolo_layer_9_loss: 3.1470 - val_loss: 16.9610 - val_yolo_layer_7_loss: 3.2181 - val_yolo_layer_8_loss: 3.2774 - val_yolo_layer_9_loss: 10.4655\n",
            "Epoch 67/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.4529 - yolo_layer_7_loss: 0.6184 - yolo_layer_8_loss: 1.7051 - yolo_layer_9_loss: 3.1294 - val_loss: 15.3339 - val_yolo_layer_7_loss: 3.0756 - val_yolo_layer_8_loss: 4.0924 - val_yolo_layer_9_loss: 8.1659\n",
            "Epoch 68/200\n",
            "168/168 [==============================] - 178s 1s/step - loss: 5.3542 - yolo_layer_7_loss: 0.5013 - yolo_layer_8_loss: 1.8477 - yolo_layer_9_loss: 3.0052 - val_loss: 17.4463 - val_yolo_layer_7_loss: 3.1814 - val_yolo_layer_8_loss: 4.1602 - val_yolo_layer_9_loss: 10.1046\n",
            "Epoch 69/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.1755 - yolo_layer_7_loss: 0.5491 - yolo_layer_8_loss: 1.3993 - yolo_layer_9_loss: 3.2270 - val_loss: 14.9361 - val_yolo_layer_7_loss: 2.2148 - val_yolo_layer_8_loss: 4.0383 - val_yolo_layer_9_loss: 8.6830\n",
            "Epoch 70/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.4381 - yolo_layer_7_loss: 0.5478 - yolo_layer_8_loss: 1.6053 - yolo_layer_9_loss: 3.2850 - val_loss: 15.6388 - val_yolo_layer_7_loss: 2.9708 - val_yolo_layer_8_loss: 3.6551 - val_yolo_layer_9_loss: 9.0130\n",
            "Epoch 71/200\n",
            "168/168 [==============================] - 180s 1s/step - loss: 5.1364 - yolo_layer_7_loss: 0.4880 - yolo_layer_8_loss: 1.5934 - yolo_layer_9_loss: 3.0550 - val_loss: 14.1902 - val_yolo_layer_7_loss: 2.4454 - val_yolo_layer_8_loss: 3.0581 - val_yolo_layer_9_loss: 8.6867\n",
            "Epoch 72/200\n",
            "168/168 [==============================] - 184s 1s/step - loss: 5.4219 - yolo_layer_7_loss: 0.7122 - yolo_layer_8_loss: 1.7250 - yolo_layer_9_loss: 2.9847 - val_loss: 15.9131 - val_yolo_layer_7_loss: 3.3852 - val_yolo_layer_8_loss: 3.7121 - val_yolo_layer_9_loss: 8.8159\n",
            "Epoch 73/200\n",
            "168/168 [==============================] - 179s 1s/step - loss: 5.2774 - yolo_layer_7_loss: 0.5097 - yolo_layer_8_loss: 1.5552 - yolo_layer_9_loss: 3.2126 - val_loss: 16.7219 - val_yolo_layer_7_loss: 3.0210 - val_yolo_layer_8_loss: 4.0854 - val_yolo_layer_9_loss: 9.6156\n",
            "Epoch 74/200\n",
            "141/168 [========================>.....] - ETA: 26s - loss: 5.8004 - yolo_layer_7_loss: 0.5930 - yolo_layer_8_loss: 1.6774 - yolo_layer_9_loss: 3.5300"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-7c227fd9a715>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                        \u001b[0mnum_experiments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                        train_from_pretrained_model=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/pretrained-yolov3.h5\")\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/__init__.py\u001b[0m in \u001b[0;36mtrainModel\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m         )\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YGzjnBR4W2H",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate model on validation images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZYrw-kwFQ5x",
        "colab_type": "code",
        "outputId": "d75e67b7-13fd-4195-acba-120266985940",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/yolo.py:24: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-001--loss-0051.291.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.0113\n",
            "mAP: 0.0113\n",
            "===============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/utils/utils.py:197: RuntimeWarning: overflow encountered in exp\n",
            "  w = anchors[2 * b + 0] * np.exp(w) / net_w # unit: image width\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "skipping the evaluation of /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-001--loss-0065.017.h5 because following exception occurred: cannot convert float infinity to integer\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-002--loss-0021.501.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.0835\n",
            "mAP: 0.0835\n",
            "===============================\n",
            "skipping the evaluation of /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-002--loss-0028.187.h5 because following exception occurred: cannot convert float infinity to integer\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-003--loss-0022.255.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.0677\n",
            "mAP: 0.0677\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-004--loss-0020.483.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.1765\n",
            "mAP: 0.1765\n",
            "===============================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/utils/utils.py:198: RuntimeWarning: overflow encountered in exp\n",
            "  h = anchors[2 * b + 1] * np.exp(h) / net_h # unit: image height\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "skipping the evaluation of /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-005--loss-0018.436.h5 because following exception occurred: cannot convert float infinity to integer\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-006--loss-0017.148.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4717\n",
            "mAP: 0.4717\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-007--loss-0015.079.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4833\n",
            "mAP: 0.4833\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-008--loss-0012.167.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.0785\n",
            "mAP: 0.0785\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-009--loss-0011.186.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.0593\n",
            "mAP: 0.0593\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-011--loss-0010.789.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4506\n",
            "mAP: 0.4506\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-013--loss-0009.529.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3653\n",
            "mAP: 0.3653\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-014--loss-0009.269.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3999\n",
            "mAP: 0.3999\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-015--loss-0009.225.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3744\n",
            "mAP: 0.3744\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-016--loss-0008.695.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3050\n",
            "mAP: 0.3050\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-018--loss-0008.117.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4197\n",
            "mAP: 0.4197\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-019--loss-0008.060.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.5190\n",
            "mAP: 0.5190\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-021--loss-0007.648.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3842\n",
            "mAP: 0.3842\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-023--loss-0007.367.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4051\n",
            "mAP: 0.4051\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-024--loss-0006.945.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3348\n",
            "mAP: 0.3348\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-027--loss-0006.263.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3794\n",
            "mAP: 0.3794\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-028--loss-0006.113.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3315\n",
            "mAP: 0.3315\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-029--loss-0005.806.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3834\n",
            "mAP: 0.3834\n",
            "===============================\n",
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-031--loss-0005.483.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.5025\n",
            "mAP: 0.5025\n",
            "===============================\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bt0PgjV9FRat",
        "colab_type": "text"
      },
      "source": [
        "# Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbTrTxdo05Fh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imageai.Detection.Custom import CustomObjectDetection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH3Ze6-r_IpR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "a17dcd0b-9be8-45a6-ed99-590d66ed3eb0"
      },
      "source": [
        "detector = CustomObjectDetection()\n",
        "detector.setModelTypeAsYOLOv3()\n",
        "detector.setModelPath(\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-054--loss-0004.711.h5\")\n",
        "detector.setJsonPath(\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\")\n",
        "detector.loadModel()\n",
        "# detections = detector.detectObjectsFromImage(input_image=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp95.jpg\", \n",
        "#                                              output_image_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp95tested.jpg\",\n",
        "#                                              minimum_percentage_probability=28)\n",
        "# for detection in detections:\n",
        "#     print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAecvXAnpnvH",
        "colab_type": "text"
      },
      "source": [
        "# Grab outputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdbD0IHfFREk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        },
        "outputId": "6e6fe021-6bfb-4ef5-bab0-5c48fda78a68"
      },
      "source": [
        "track_info = []\n",
        "for i in range(91,103):\n",
        "  detections = detector.detectObjectsFromImage(input_image=f\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp{i}.jpg\", \n",
        "                                               output_image_path=f\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp{i}tested.jpg\",\n",
        "                                               minimum_percentage_probability=25)\n",
        "  track_info.append(detections)\n",
        "  for detection in detections:\n",
        "    print(detection[\"name\"], \" : \", detection[\"percentage_probability\"], \" : \", detection[\"box_points\"])\n",
        "  print('\\n new pic')\n",
        "# output images will already be put into path  "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "toiletpaper  :  33.870479464530945  :  [33, 121, 111, 209]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  48.853832483291626  :  [81, 63, 174, 154]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  25.53916573524475  :  [161, 93, 250, 168]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  29.890868067741394  :  [659, 218, 762, 348]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  89.11226987838745  :  [205, 135, 231, 174]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  30.941888689994812  :  [111, 2, 184, 65]\n",
            "toiletpaper  :  67.76886582374573  :  [144, 91, 160, 124]\n",
            "toiletpaper  :  71.46404385566711  :  [111, 111, 133, 136]\n",
            "toiletpaper  :  81.8886935710907  :  [136, 111, 156, 138]\n",
            "toiletpaper  :  88.23094367980957  :  [94, 133, 118, 163]\n",
            "toiletpaper  :  88.41504454612732  :  [124, 134, 147, 165]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  55.79391121864319  :  [56, 36, 161, 160]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  34.75150465965271  :  [81, 3, 230, 182]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  55.0812304019928  :  [40, 47, 154, 161]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  61.552029848098755  :  [65, 66, 205, 170]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  49.92387592792511  :  [36, 20, 329, 324]\n",
            "toiletpaper  :  29.681453108787537  :  [349, 0, 609, 404]\n",
            "toiletpaper  :  55.80770969390869  :  [161, 110, 484, 466]\n",
            "\n",
            " new pic\n",
            "toiletpaper  :  50.175195932388306  :  [688, 230, 1245, 952]\n",
            "\n",
            " new pic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7_k_lKRprlg",
        "colab_type": "text"
      },
      "source": [
        "# Use output info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bxs6p39mq2MW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a956cuW6EFiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j = 91\n",
        "for i in track_info:\n",
        "  current_img = cv2.imread(f\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp{j}.jpg\", cv2.IMREAD_UNCHANGED)\n",
        "  for box in i:\n",
        "    \n",
        "    cv2.rectangle(img = current_img,\n",
        "                  rec = (box['box_points'][0], box['box_points'][1], box['box_points'][2]-box['box_points'][0], box['box_points'][3]-box['box_points'][1]), # integer argument expected\n",
        "                  color = (255,200,0),\n",
        "                  thickness = 1)\n",
        "\n",
        "    cv2.putText(img = current_img,\n",
        "                text = f\"{box['name']}\", # gonna be only toiletpaper so...\n",
        "                color = (0,0,0), \n",
        "                org = (box['box_points'][0] + 3, box['box_points'][1] + 11), \n",
        "                fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                fontScale = .3,\n",
        "                thickness = 1)\n",
        "\n",
        "    cv2.putText(img = current_img, \n",
        "                text = f\"{round(box['percentage_probability'], 4):.2f}%\", \n",
        "                color = (0,0,0), \n",
        "                org = (box['box_points'][0] + 2, box['box_points'][1] + 26), \n",
        "                fontFace = cv2.FONT_HERSHEY_SIMPLEX, \n",
        "                fontScale = .3,\n",
        "                thickness = 1)\n",
        "    \n",
        "    cv2.imwrite(f\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/validation/images/tp{j}tested.jpg\", current_img)\n",
        "  j += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmRA-HHGIcMG",
        "colab_type": "text"
      },
      "source": [
        "# Rest of evaluation because not enough RAM to do in one go"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi3clSwZqUUH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "5f15cc42-752d-40ce-ca24-642b9615712c"
      },
      "source": [
        "trainer = DetectionModelTrainer()\n",
        "trainer.setModelTypeAsYOLOv3()\n",
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-032--loss-0005.313.h5\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/imageai/Detection/Custom/yolo.py:24: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-032--loss-0005.313.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4049\n",
            "mAP: 0.4049\n",
            "===============================\n",
            "[{'model_file': '/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-032--loss-0005.313.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.5, 'average_precision': {'toiletpaper': 0.40489574854899935}, 'map': 0.40489574854899935}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etOGpUBI2tLG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "d311bd22-1b2f-44e8-9e3c-588a870753d7"
      },
      "source": [
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-036--loss-0005.166.h5\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-036--loss-0005.166.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.4027\n",
            "mAP: 0.4027\n",
            "===============================\n",
            "[{'model_file': '/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-036--loss-0005.166.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.5, 'average_precision': {'toiletpaper': 0.4027278052045854}, 'map': 0.4027278052045854}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaCdZZKQEu0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "677bcd6f-fa41-49c0-ff6c-ada285b90fb5"
      },
      "source": [
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-038--loss-0005.145.h5\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-038--loss-0005.145.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3800\n",
            "mAP: 0.3800\n",
            "===============================\n",
            "[{'model_file': '/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-038--loss-0005.145.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.5, 'average_precision': {'toiletpaper': 0.3799682034976153}, 'map': 0.3799682034976153}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oxkwo6yfEu9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "6262d17c-9b7a-4a1e-c3ca-175efd4f2f7e"
      },
      "source": [
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-044--loss-0005.130.h5\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-044--loss-0005.130.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3809\n",
            "mAP: 0.3809\n",
            "===============================\n",
            "[{'model_file': '/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-044--loss-0005.130.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.5, 'average_precision': {'toiletpaper': 0.38092209856915743}, 'map': 0.38092209856915743}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74ucSVPpEvGc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "a0b2122b-514f-4a62-e7cb-fb99c630489c"
      },
      "source": [
        "trainer.setDataDirectory(data_directory=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper\")\n",
        "metrics = trainer.evaluateModel(model_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-054--loss-0004.711.h5\", \n",
        "                                json_path=\"/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/json/detection_config.json\", \n",
        "                                iou_threshold=0.5, \n",
        "                                object_threshold=0.3, \n",
        "                                nms_threshold=0.5)\n",
        "print(metrics)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Model evaluation....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:310: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
            "  warnings.warn('No training configuration found in save file: '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model File:  /content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-054--loss-0004.711.h5 \n",
            "\n",
            "Using IoU :  0.5\n",
            "Using Object Threshold :  0.3\n",
            "Using Non-Maximum Suppression :  0.5\n",
            "toiletpaper: 0.3801\n",
            "mAP: 0.3801\n",
            "===============================\n",
            "[{'model_file': '/content/drive/My Drive/Final Project trying to improve model/obj detect 2/toiletpaper/models/detection_model-ex-054--loss-0004.711.h5', 'using_iou': 0.5, 'using_object_threshold': 0.3, 'using_non_maximum_suppression': 0.5, 'average_precision': {'toiletpaper': 0.380144850733086}, 'map': 0.380144850733086}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2nu4S-mEvO7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1oREYQYEvXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdy7Gn8CEvgJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}